{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import hyperparams as hp\n",
    "import numpy as np\n",
    "import math\n",
    "import glu\n",
    "import positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Network\n",
    "    \"\"\"\n",
    "    def __init__(self, para):\n",
    "        \"\"\"\n",
    "        :param para: dictionary that contains all parameters\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.alpha = nn.Parameter(t.ones(1))\n",
    "        \n",
    "        self.emb_phone = nn.Embedding(para['phone_size'], para['emb_dim'])\n",
    "        #full connected\n",
    "        self.fc_1 = nn.Linear(para['emb_dim'], para['GLU_in_dim'])\n",
    "        \n",
    "        self.GLU = glu.GLU(para['num_layers'], para['hidden_size'], para['kernel_size'], para['dropout'], para['GLU_in_dim'])\n",
    "        \n",
    "        self.fc_2 = nn.Linear(para['hidden_size'], para['emb_dim'])\n",
    "        \n",
    "    def refine(self, align_phone):\n",
    "        '''filter silence phone and repeat phone'''\n",
    "        out = []\n",
    "        length = []\n",
    "        batch_size = align_phone.shape[0]\n",
    "        max_length = align_phone.shape[1]\n",
    "        before = 0\n",
    "        for i in range(batch_size):\n",
    "            line = []\n",
    "            for j in range(max_length):\n",
    "                if align_phone[i][j] == 1 or align_phone[i][j] == 0:      #silence phone or padding\n",
    "                    continue\n",
    "                elif align_phone[i][j] == before:   #the same with the former phone\n",
    "                    continue\n",
    "                else:\n",
    "                    before = align_phone[i][j]\n",
    "                    line.append(before)\n",
    "            out.append(line)\n",
    "            length.append(len(line))\n",
    "        \n",
    "        #pad 0\n",
    "        seq_length = max(length)\n",
    "        Data = np.zeros((batch_size, seq_length))\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_length):\n",
    "                if j < len(out[i]):\n",
    "                    Data[i][j] = out[i][j]\n",
    "                    \n",
    "        return torch.from_numpy(Data).type(torch.LongTensor)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input dim: [batch_size, text_phone_length]\n",
    "        output dim : [batch_size, text_phone_length, embedded_dim]\n",
    "        \"\"\"\n",
    "        input = self.refine(input)\n",
    "        print(input)\n",
    "        embedded_phone = self.emb_phone(input)    # [src len, batch size, emb dim]\n",
    "        print(embedded_phone.shape,embedded_phone)\n",
    "        glu_out = self.GLU(self.fc_1(embedded_phone))\n",
    "        print(glu_out.shape)\n",
    "        glu_out = self.fc_2(torch.transpose(glu_out, 1, 2))\n",
    "        print(glu_out.shape,glu_out)\n",
    "        out = embedded_phone + glu_out\n",
    "        print(out.shape,out)\n",
    "        out = out *  math.sqrt(0.5)\n",
    "        print(out.shape,out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Postnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Postnet\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder_Postnet, self, seq_length).__init__()\n",
    "        #length of sequence = number of frames\n",
    "        self.fc = nn.Linear(seq_length, seq_length)\n",
    "         \n",
    "    def aligner(encoder_out, align_phone):\n",
    "        return\n",
    "        \n",
    "    def forward(self, encoder_out, align_phone, pitch, beats):\n",
    "        aligner_out = aligner(encoder_out, align_phone)\n",
    "        pitch = self.fc(pitch)\n",
    "        out = aligner_out + pitch\n",
    "        beats_avg = len(beats) / sum(beats)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5, 6, 0],\n",
      "        [4, 2, 3, 7]])\n",
      "torch.Size([2, 4, 256]) tensor([[[-0.1056,  0.4499,  1.0647,  ..., -0.1748,  0.2333, -0.7960],\n",
      "         [ 1.5063, -0.0484, -1.4599,  ..., -1.3495, -0.3319, -0.4540],\n",
      "         [ 0.1539, -0.7001,  1.2649,  ..., -0.5915,  0.1337,  2.0040],\n",
      "         [ 0.6995,  0.0996,  0.2429,  ..., -0.4456, -0.3416,  0.9352]],\n",
      "\n",
      "        [[-1.7274, -0.8727, -0.9020,  ...,  0.7828,  0.6503, -1.3442],\n",
      "         [ 0.5065, -1.0624,  0.6139,  ..., -0.8265, -1.2438,  1.8009],\n",
      "         [-0.1056,  0.4499,  1.0647,  ..., -0.1748,  0.2333, -0.7960],\n",
      "         [-0.3482,  0.5405, -0.1930,  ..., -1.2947, -0.1482, -0.0392]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([2, 64, 4])\n",
      "torch.Size([2, 4, 256]) tensor([[[ 1.0397e-01,  9.0222e-02, -1.1796e-01,  ..., -2.1993e-01,\n",
      "          -1.1756e-01, -1.4229e-01],\n",
      "         [-8.7076e-02, -1.5004e-01,  5.8925e-02,  ...,  1.3462e-01,\n",
      "          -9.9201e-03, -1.3423e-01],\n",
      "         [-1.4504e-01, -4.9546e-02, -1.4552e-01,  ...,  1.4421e-01,\n",
      "          -2.3745e-01, -1.1906e-01],\n",
      "         [ 9.0925e-02, -1.5017e-02, -6.8574e-03,  ...,  1.3796e-01,\n",
      "          -1.9589e-01, -3.3855e-01]],\n",
      "\n",
      "        [[-2.5545e-01,  7.0038e-02, -1.1851e-01,  ..., -1.5713e-01,\n",
      "           4.1973e-02, -5.9311e-02],\n",
      "         [-3.5138e-01, -1.1986e-01, -3.1410e-02,  ..., -1.6514e-01,\n",
      "          -1.3959e-01, -3.1711e-02],\n",
      "         [-7.3484e-02,  9.0957e-03, -1.7167e-01,  ..., -7.3311e-02,\n",
      "           5.6952e-02, -4.5173e-02],\n",
      "         [-1.8181e-01,  9.8065e-05, -9.8661e-03,  ..., -2.2160e-01,\n",
      "           8.9024e-03, -8.2073e-02]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 256]) tensor([[[-1.6560e-03,  5.4014e-01,  9.4673e-01,  ..., -3.9476e-01,\n",
      "           1.1578e-01, -9.3826e-01],\n",
      "         [ 1.4192e+00, -1.9843e-01, -1.4010e+00,  ..., -1.2148e+00,\n",
      "          -3.4185e-01, -5.8828e-01],\n",
      "         [ 8.8734e-03, -7.4967e-01,  1.1193e+00,  ..., -4.4729e-01,\n",
      "          -1.0377e-01,  1.8850e+00],\n",
      "         [ 7.9039e-01,  8.4539e-02,  2.3608e-01,  ..., -3.0765e-01,\n",
      "          -5.3751e-01,  5.9666e-01]],\n",
      "\n",
      "        [[-1.9829e+00, -8.0262e-01, -1.0205e+00,  ...,  6.2570e-01,\n",
      "           6.9223e-01, -1.4035e+00],\n",
      "         [ 1.5511e-01, -1.1822e+00,  5.8249e-01,  ..., -9.9166e-01,\n",
      "          -1.3834e+00,  1.7692e+00],\n",
      "         [-1.7911e-01,  4.5902e-01,  8.9301e-01,  ..., -2.4814e-01,\n",
      "           2.9029e-01, -8.4114e-01],\n",
      "         [-5.3002e-01,  5.4058e-01, -2.0290e-01,  ..., -1.5163e+00,\n",
      "          -1.3933e-01, -1.2131e-01]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 256]) tensor([[[-1.1710e-03,  3.8194e-01,  6.6944e-01,  ..., -2.7914e-01,\n",
      "           8.1869e-02, -6.6345e-01],\n",
      "         [ 1.0035e+00, -1.4031e-01, -9.9063e-01,  ..., -8.5903e-01,\n",
      "          -2.4172e-01, -4.1598e-01],\n",
      "         [ 6.2744e-03, -5.3010e-01,  7.9148e-01,  ..., -3.1628e-01,\n",
      "          -7.3379e-02,  1.3329e+00],\n",
      "         [ 5.5889e-01,  5.9778e-02,  1.6693e-01,  ..., -2.1754e-01,\n",
      "          -3.8008e-01,  4.2190e-01]],\n",
      "\n",
      "        [[-1.4021e+00, -5.6754e-01, -7.2159e-01,  ...,  4.4243e-01,\n",
      "           4.8948e-01, -9.9244e-01],\n",
      "         [ 1.0968e-01, -8.3596e-01,  4.1189e-01,  ..., -7.0121e-01,\n",
      "          -9.7823e-01,  1.2510e+00],\n",
      "         [-1.2665e-01,  3.2457e-01,  6.3146e-01,  ..., -1.7546e-01,\n",
      "           2.0527e-01, -5.9477e-01],\n",
      "         [-3.7478e-01,  3.8225e-01, -1.4347e-01,  ..., -1.0722e+00,\n",
      "          -9.8522e-02, -8.5776e-02]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "para = {'phone_size':67, 'emb_dim':256, 'GLU_in_dim':64, 'num_layers':6, 'kernel_size':3, 'hidden_size':64, 'dropout':0.1 }\n",
    "encoder = Encoder(para)\n",
    "phone = torch.tensor([[1,3,3,3,3,5,5,6,0,0,0],[1,1,1,4,2,2,2,3,7,1,1]])\n",
    "out = encoder(phone)\n",
    "#print(out.shape,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 2.2222, 0.0000, 0.0000, 0.0000, 2.2222, 0.0000, 1.1111]],\n",
      "\n",
      "        [[0.9350, 0.0000, 0.1109, 2.2167, 1.1222, 1.1111, 1.1122, 1.1111]]])\n"
     ]
    }
   ],
   "source": [
    "beats = torch.tensor([[[0,1,0,0,0,1,0,0]],[[0,0,0,1,1,0,1,0]]])\n",
    "pos = positional_encoding.PositionalEncoding(8)\n",
    "out = pos(beats)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
