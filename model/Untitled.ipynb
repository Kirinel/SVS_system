{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import hyperparams as hp\n",
    "import numpy as np\n",
    "import math\n",
    "import glu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Network\n",
    "    \"\"\"\n",
    "    def __init__(self, para):\n",
    "        \"\"\"\n",
    "        :param para: dictionary that contains all parameters\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.alpha = nn.Parameter(t.ones(1))\n",
    "        \n",
    "        self.emb_phone = nn.Embedding(para['phone_size'], para['emb_dim'])\n",
    "        #full connected\n",
    "        self.fc_1 = nn.Linear(para['emb_dim'], para['GLU_in_dim'])\n",
    "        \n",
    "        self.GLU = glu.GLU(para['num_layers'], para['hidden_size'], para['kernel_size'], para['dropout'], para['GLU_in_dim'])\n",
    "        \n",
    "        self.fc_2 = nn.Linear(para['hidden_size'], para['emb_dim'])\n",
    "        \n",
    "    def refine(self, align_phone):\n",
    "        '''filter silence phone and repeat phone'''\n",
    "        out = []\n",
    "        length = []\n",
    "        batch_size = align_phone.shape[0]\n",
    "        max_length = align_phone.shape[1]\n",
    "        before = 0\n",
    "        for i in range(batch_size):\n",
    "            line = []\n",
    "            for j in range(max_length):\n",
    "                if align_phone[i][j] == 1 or align_phone[i][j] == 0:      #silence phone or padding\n",
    "                    continue\n",
    "                elif align_phone[i][j] == before:   #the same with the former phone\n",
    "                    continue\n",
    "                else:\n",
    "                    before = align_phone[i][j]\n",
    "                    line.append(before)\n",
    "            out.append(line)\n",
    "            length.append(len(line))\n",
    "        \n",
    "        #pad 0\n",
    "        seq_length = max(length)\n",
    "        Data = np.zeros((batch_size, seq_length))\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_length):\n",
    "                if j < len(out[i]):\n",
    "                    Data[i][j] = out[i][j]\n",
    "                    \n",
    "        return torch.from_numpy(Data).type(torch.LongTensor)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input dim: [batch_size, text_phone_length]\n",
    "        output dim : [batch_size, text_phone_length, embedded_dim]\n",
    "        \"\"\"\n",
    "        input = self.refine(input)\n",
    "        print(input)\n",
    "        embedded_phone = self.emb_phone(input)    # [src len, batch size, emb dim]\n",
    "        print(embedded_phone.shape,embedded_phone)\n",
    "        glu_out = self.GLU(self.fc_1(embedded_phone))\n",
    "        print(glu_out.shape)\n",
    "        glu_out = self.fc_2(torch.transpose(glu_out, 1, 2))\n",
    "        print(glu_out.shape,glu_out)\n",
    "        out = embedded_phone + glu_out\n",
    "        print(out.shape,out)\n",
    "        out = out *  math.sqrt(0.5)\n",
    "        print(out.shape,out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 5, 6, 0],\n",
      "        [4, 2, 3, 7]])\n",
      "torch.Size([2, 4, 256]) tensor([[[-0.1634, -1.1551,  2.1703,  ...,  0.0700, -0.9398,  0.2773],\n",
      "         [-0.0096,  0.2066, -1.1827,  ...,  0.0633, -0.4822,  1.9444],\n",
      "         [-0.1730,  0.8219, -0.7822,  ...,  1.5981,  0.8638, -0.0104],\n",
      "         [ 1.1558, -0.1630, -0.2812,  ..., -1.0534, -0.5543,  2.0829]],\n",
      "\n",
      "        [[ 1.2973, -0.3188, -0.8178,  ..., -0.3757,  1.2727, -0.7920],\n",
      "         [ 0.2669, -0.5398, -1.0456,  ...,  0.2583,  0.4187, -0.0991],\n",
      "         [-0.1634, -1.1551,  2.1703,  ...,  0.0700, -0.9398,  0.2773],\n",
      "         [-2.5658, -0.5674,  0.3152,  ..., -0.3762, -0.8581,  2.1603]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([2, 64, 4])\n",
      "torch.Size([2, 4, 256]) tensor([[[ 0.1461, -0.0676,  0.0320,  ..., -0.0243,  0.0367,  0.0234],\n",
      "         [ 0.2043, -0.1260,  0.0625,  ..., -0.0467,  0.0961, -0.2177],\n",
      "         [ 0.0364, -0.0187,  0.0945,  ..., -0.1370, -0.0689, -0.1879],\n",
      "         [ 0.1482, -0.1088,  0.2525,  ..., -0.0798,  0.0931, -0.0503]],\n",
      "\n",
      "        [[-0.0861, -0.1805,  0.1777,  ...,  0.0018,  0.0970, -0.0556],\n",
      "         [ 0.0154, -0.1670, -0.0165,  ..., -0.0499, -0.1644, -0.0062],\n",
      "         [ 0.1143, -0.0325, -0.0293,  ..., -0.0082,  0.0590, -0.0130],\n",
      "         [ 0.1871, -0.1582,  0.1480,  ..., -0.0915,  0.1358, -0.1348]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 256]) tensor([[[-0.0173, -1.2227,  2.2023,  ...,  0.0458, -0.9031,  0.3007],\n",
      "         [ 0.1947,  0.0806, -1.1202,  ...,  0.0166, -0.3861,  1.7267],\n",
      "         [-0.1366,  0.8032, -0.6878,  ...,  1.4611,  0.7949, -0.1983],\n",
      "         [ 1.3040, -0.2718, -0.0287,  ..., -1.1332, -0.4612,  2.0326]],\n",
      "\n",
      "        [[ 1.2112, -0.4994, -0.6401,  ..., -0.3739,  1.3696, -0.8477],\n",
      "         [ 0.2823, -0.7068, -1.0620,  ...,  0.2084,  0.2543, -0.1054],\n",
      "         [-0.0491, -1.1876,  2.1410,  ...,  0.0618, -0.8808,  0.2643],\n",
      "         [-2.3787, -0.7257,  0.4632,  ..., -0.4677, -0.7223,  2.0255]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 256]) tensor([[[-0.0123, -0.8646,  1.5573,  ...,  0.0324, -0.6386,  0.2127],\n",
      "         [ 0.1377,  0.0570, -0.7921,  ...,  0.0117, -0.2730,  1.2210],\n",
      "         [-0.0966,  0.5679, -0.4863,  ...,  1.0332,  0.5621, -0.1402],\n",
      "         [ 0.9221, -0.1922, -0.0203,  ..., -0.8013, -0.3261,  1.4373]],\n",
      "\n",
      "        [[ 0.8564, -0.3531, -0.4526,  ..., -0.2644,  0.9685, -0.5994],\n",
      "         [ 0.1996, -0.4998, -0.7510,  ...,  0.1474,  0.1798, -0.0745],\n",
      "         [-0.0347, -0.8398,  1.5139,  ...,  0.0437, -0.6229,  0.1869],\n",
      "         [-1.6820, -0.5131,  0.3275,  ..., -0.3307, -0.5107,  1.4322]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "para = {'phone_size':67, 'emb_dim':256, 'GLU_in_dim':64, 'num_layers':6, 'kernel_size':3, 'hidden_size':64, 'dropout':0.1 }\n",
    "encoder = Encoder(para)\n",
    "phone = torch.tensor([[1,3,3,3,3,5,5,6,0,0,0],[1,1,1,4,2,2,2,3,7,1,1]])\n",
    "out = encoder(phone)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
